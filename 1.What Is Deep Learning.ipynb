{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Deep Learning?\n",
    "- Cut through the noise to differentiate between the press perception and world-changing developments.\n",
    "\n",
    "- Questions to be tackled:\n",
    "  - What has Deep Learning achieved so far?\n",
    "  - How significant is it?\n",
    "  - Where are we headed next?\n",
    "  - Should we believe the hype?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Intelligence, Machine Learning and Deep Learning\n",
    "\n",
    "|<img alt=\"AI, ML and DL Venn Diagram\" src=\"./images/AI_ML_DL_venn.png\" width=400/>\n",
    "|:----:|\n",
    "|*Artificial Intelligence, Machine Learning and Deep Learning*|\n",
    "\n",
    "<br><br>\n",
    "\n",
    "|Artificial Intelligence|Machine Learning|Deep Learning|\n",
    "|:----------------------|:---------------|:------------|\n",
    "|<ul><li>Started in 1950s</li><li>A concise definition: *The effort to automate intellectual tasks performed by human beings.*</li><li>Superset of Machine Learning and Deep Learning, which also includes techniques that doesn't involve learning.</li><li>Symbolic AI:<ul><li>Handcrafting a large number of rules to manipulate knowledge</li><li>1950s to 1980s</li><li>Peaked in popularity in 1980s, during *expert systems* boom</li><li>Difficult to use for fuzzy problems, like Image Classification and Speech Recognition.</li><li>Hence Machine Learning arose.</li></ul></li></ul>|<ul><li>Ada Lovelace remark on *Analytical Engine*: \"The Analytical Engine has no pretensions whatever to originate anything. It can do whatever we know how to order it to perform... Its province is to assist us in making available what we're already acquainted with.\"</li><li>Alan Turing when introducing *Truing Test* quoted above remark as \"Lady Lovelace's Objection\" and came to the conclusion that general purpose computers are capable of learning and originality.</li><li>Machine Learning arises from questions:<ul><li>Could a computer go beyond what we know how to order it to perform and learn on its own how to perform a specified task?</li><li> Could a computer suprise us?</li><li>Can it define rules on its own by looking at data?</li></ul><br><img alt=\"Classical Programming vs Machine Learning\" src=\"./images/classicalprogramming-vs-ml.png\" style=\"align:center\"/></li><li>It is trained rather than explicitly programmed</li><li>It finds statisical structure in the data examples presented to it.</li><li>Tightly linked to mathematical statistics, but differs from statistics in several ways:<ul><li>ML deals with more complex data, in large amounts, for which classical stats like Bayesian Analysis would be impractical.</li></ul></li></ul>|<ul><li>Next level of Machine Learning</li><li>Better way to find hidden representation in the data.</li><li>Learn about data reprsentation in the section below.</li><li>*Deep* in deep learning stands for finding input to output map using successive layers of increasingly meaningful representations</li><li>No of layers in the model is called *depth* of the model</li><li>Other apt names for the field could have been:<ul><li>Layered Representations Learning</li><li>Hierarchical Representations Learning</li></ul></li><li>Other ML approaches focuses on using either one or two layers of representations, while Deep Learning can have 100s of layers of representations. Hence, ML approaches are sometimes called *shallow learning*</li><li>Most often the layered representations of Deep Learning are achieved using *neural networks*</li><li>Term \"Neural Networks\" came from neural biology and many of its characteristics are taken from \"our understanding\" of human brain. But it isn't a model of a human brain!</li><li>There is no evidence that the brain uses learning algorithms, used by moder Deep Networks!</li></ul>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Representation Learning\n",
    "- Here we will learn about differences between Deep Learning and other Machine Learning algorithms.\n",
    "- To do ML we need 3 things:\n",
    "  - Input data points. Example: Images of Dog and Cat\n",
    "  - Examples of expected outputs. Example: labels stating whether image is that of dog or cat.\n",
    "  - Way to measure whether the algorithm is doing a good job. This is important to measure the distance between the expected output and where the algorithm is right now!\n",
    "\n",
    "- An ML model (algorithm), transform input into meaningful outputs.\n",
    "- This is done by a process that is \"learned\" by exposure to known outputs.\n",
    "- Central problem in ML and DL - *meaningfully transform data*. Change representation of input data to something that is more closer to the output.\n",
    "- *Representation* - different way to look at data.\n",
    "- An image can be represented using RGB or HSV\n",
    "\n",
    "Let's take a concrete example!\n",
    "\n",
    "![Data Representation Example](./images/data_representation_example.png)\n",
    "\n",
    "\n",
    "Now we can use $x>0$ to define blue class and $x<=0$ to define red class\n",
    "\n",
    "- Machine Learning algorithms perform these kinds of transformations automatically!\n",
    "- *Learning* in the context of Machine Learning stands for automatic search for better representations.\n",
    "- Transformations performed by Machine Learning algorithms can be:\n",
    "  - Coordinate Changes\n",
    "  - Linear Projections (which may destroy some info.)\n",
    "  - Translations\n",
    "  - Nonlinear operations (example, select all points where x>0)\n",
    "\n",
    "- ML algorithms are not very \"creative\" at finding best transformations. They search for the best combinations of transformations in a pre-defined set of operations, called a *hypothesis space*.\n",
    "\n",
    "- So what makes Deep Learning Special?\n",
    "  - Deep Learning tries to find layered hidden representation, whereas ML only finds shallow representations.\n",
    "  - ML algorithms are not very good at finding complex representation as in images and speech, whereas we can reach human level performance using Deep Networks. This is due their layered representations!\n",
    "  - Think of Neural Networks as a multi-stage information distillation operation, where information goes through successive filters and comes out increasingly *purified*!\n",
    "\n",
    "![deep representations learnt by model](./images/deep_representations_learnt_by_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding How Deep Learning Works!\n",
    "- Here we will see how Deep Networks map input to desired output using simple data layered transformations and their exposure to examples of input-target pairs.\n",
    "- Transformations performed by a layer are *parameterized* by the *weights* it holds. *Weights* are a bunch of numbers, a.k.a. *parameters*.\n",
    "- *Learning* means finding particular sets of *weights* such that network maps inputs correctly (read optimally) to associated targets.\n",
    "\n",
    "![](./images/nn_parameterized_by_weights.png)\n",
    "\n",
    "- A Deep Network can contains 10s of millions of parameters, changing one can affect behavior of others, finding the set of weights can be a daunting task.\n",
    "\n",
    "- To control the output of a neural network, we must have a sense of how far the current output is from desired output. This distance measure is called *loss function* or *objective function*.\n",
    "\n",
    "![](./images/loss_measures_quality_of_output.png)\n",
    "\n",
    "- The trick is to use the loss function's distance score as a feedback signal to adjust the weights a little, in a direction that will lower the loss score for the current example. This adjustment is the job of an *optimizer*, which implements *Backpropagation Algorithm* - the central algorithm in Deep Learning.\n",
    "\n",
    "![](./images/loss_used_as_feedback_signal.png)\n",
    "\n",
    "- Initially the weights are assigned randomly, so naturally the output will be much far from required.\n",
    "- With each example shown the network improves by adjusting its weights to perform better on the given example. This is called *training loop*. Final result, is a *trained network*, whose weights produce outputs as close to targets as close they can be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Deep Learning has achieved si fat\n",
    "- Near human level image classification\n",
    "- Near human level speech recognition\n",
    "- Near human level handwritting transcription\n",
    "- Improved Machine Translation\n",
    "- Improved text-to-speech conversion\n",
    "- Digital assistants such as Google Now and Amazon Alexa\n",
    "- Near human level autonomous driving\n",
    "- Improved ad targeting, as used by Google, Baidu and Bing\n",
    "- Improved search results on the web\n",
    "- Ability to answer natural language questions\n",
    "- Superhuman Go Playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
